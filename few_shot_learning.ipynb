{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9cUWSLMqTnV+PvT0KOict",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moeinset/few_shot_learning/blob/main/few_shot_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpDIs79ZThrh"
      },
      "outputs": [],
      "source": [
        "!pip install easyfsl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import Omniglot\n",
        "from torchvision.models import resnet18\n",
        "from tqdm import tqdm\n",
        "\n",
        "from easyfsl.samplers import TaskSampler\n",
        "from easyfsl.utils import plot_images, sliding_average\n"
      ],
      "metadata": {
        "id": "cZxa3jLOUDz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup device-agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "NSMAuLp9U4y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "# Setup path to data folder\n",
        "data_path = Path(\"data/\")\n",
        "image_path = data_path / \"tumor_stroma_lympho\"\n",
        "\n",
        "# If the image folder doesn't exist, download it and prepare it... \n",
        "if image_path.is_dir():\n",
        "    print(f\"{image_path} directory exists.\")\n",
        "else:\n",
        "    print(f\"Did not find {image_path} directory, creating one...\")\n",
        "    image_path.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Download data\n",
        "    with open(data_path / \"tumor_stroma_lympho.zip\", \"wb\") as f:\n",
        "        request = requests.get(\"https://cdn-130.anonfiles.com/8ep9p1F8y5/04b33f1f-1667216246/tumor_stroma_lympho.zip\")\n",
        "        print(\"Downloading ...\")\n",
        "        f.write(request.content)\n",
        "\n",
        "    # Unzip data\n",
        "    with zipfile.ZipFile(data_path / \"tumor_stroma_lympho.zip\", \"r\") as zip_ref:\n",
        "        print(\"Unzipping data...\") \n",
        "        zip_ref.extractall(image_path)"
      ],
      "metadata": {
        "id": "kAujP3THU70I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def walk_through_dir(dir_path):\n",
        "  \"\"\"\n",
        "  Walks through dir_path returning its contents.\n",
        "  Args:\n",
        "    dir_path (str or pathlib.Path): target directory\n",
        "  \n",
        "  Returns:\n",
        "    A print out of:\n",
        "      number of subdiretories in dir_path\n",
        "      number of images (files) in each subdirectory\n",
        "      name of each subdirectory\n",
        "  \"\"\"\n",
        "  for dirpath, dirnames, filenames in os.walk(dir_path):\n",
        "    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")"
      ],
      "metadata": {
        "id": "VyIn-1DHVAyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "walk_through_dir(image_path)"
      ],
      "metadata": {
        "id": "-1h0rxJAVCmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup train and testing paths\n",
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"test\"\n",
        "\n",
        "train_dir, test_dir"
      ],
      "metadata": {
        "id": "bPUTf3OqVGue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "# Set seed\n",
        "#random.seed(42) \n",
        "\n",
        "# 1. Get all image paths (* means \"any combination\")\n",
        "image_path_list = list(image_path.glob(\"*/*/*.tif\"))\n",
        "\n",
        "# 2. Get random image path\n",
        "random_image_path = random.choice(image_path_list)\n",
        "\n",
        "# 3. Get image class from path name (the image class is the name of the directory where the image is stored)\n",
        "image_class = random_image_path.parent.stem\n",
        "\n",
        "# 4. Open image\n",
        "img = Image.open(random_image_path)\n",
        "\n",
        "# 5. Print metadata\n",
        "print(f\"Random image path: {random_image_path}\")\n",
        "print(f\"Image class: {image_class}\")\n",
        "print(f\"Image height: {img.height}\") \n",
        "print(f\"Image width: {img.width}\")\n",
        "img\n",
        "     "
      ],
      "metadata": {
        "id": "l4X34hEcVLK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a set of pretrained model weights\n",
        "weights = torchvision.models.ResNet18_Weights.DEFAULT # .DEFAULT = best available weights from pretraining on ImageNet\n",
        "# Get the transforms used to create our pretrained weights\n",
        "auto_transforms = weights.transforms()\n"
      ],
      "metadata": {
        "id": "gZFMRkLWVio0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image_size = 28\n",
        "\n",
        "# # NB: background=True selects the train set, background=False selects the test set\n",
        "# # It's the nomenclature from the original paper, we just have to deal with it\n",
        "\n",
        "# train_set = Omniglot(\n",
        "#     root=\"./data\",\n",
        "#     background=True,\n",
        "#     transform=transforms.Compose(\n",
        "#         [\n",
        "#             transforms.Grayscale(num_output_channels=3),\n",
        "#             transforms.RandomResizedCrop(image_size),\n",
        "#             transforms.RandomHorizontalFlip(),\n",
        "#             transforms.ToTensor(),\n",
        "#         ]\n",
        "#     ),\n",
        "#     download=True,\n",
        "# )\n",
        "# test_set = Omniglot(\n",
        "#     root=\"./data\",\n",
        "#     background=False,\n",
        "#     transform=transforms.Compose(\n",
        "#         [\n",
        "#             # Omniglot images have 1 channel, but our model will expect 3-channel images\n",
        "#             transforms.Grayscale(num_output_channels=3),\n",
        "#             transforms.Resize([int(image_size * 1.15), int(image_size * 1.15)]),\n",
        "#             transforms.CenterCrop(image_size),\n",
        "#             transforms.ToTensor(),\n",
        "#         ]\n",
        "#     ),\n",
        "#     download=True,\n",
        "# )\n"
      ],
      "metadata": {
        "id": "sVscE7RJX8ZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Create training and testing DataLoaders as well as get a list of class names\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
        "                                                                               test_dir=test_dir,\n",
        "                                                                               train_transform=auto_transforms, # perform same data transforms on our own data as the pretrained model\n",
        "                                                                               test_transform=auto_transforms,\n",
        "                                                                               batch_size=32) # set mini-batch size to 32\n",
        "\n",
        "train_dataloader, test_dataloader, class_names\n",
        "     \n"
      ],
      "metadata": {
        "id": "EEa4BWPjVnfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = torchvision.models.ResNet18_Weights.DEFAULT # .DEFAULT = best available weights \n",
        "model = torchvision.models.resnet.ResNet(weights=weights)"
      ],
      "metadata": {
        "id": "s_MumSEFVrXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print a summary using torchinfo (uncomment for actual output)\n",
        "summary(model=model, \n",
        "        input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n",
        "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"]\n",
        ") \n",
        "     "
      ],
      "metadata": {
        "id": "dSDVsR1bVxKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PrototypicalNetworks(nn.Module):\n",
        "    def __init__(self, backbone: nn.Module):\n",
        "        super(PrototypicalNetworks, self).__init__()\n",
        "        self.backbone = backbone\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        support_images: torch.Tensor,\n",
        "        support_labels: torch.Tensor,\n",
        "        query_images: torch.Tensor,\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Predict query labels using labeled support images.\n",
        "        \"\"\"\n",
        "        # Extract the features of support and query images\n",
        "        z_support = self.backbone.forward(support_images)\n",
        "        z_query = self.backbone.forward(query_images)\n",
        "\n",
        "        # Infer the number of different classes from the labels of the support set\n",
        "        n_way = len(torch.unique(support_labels))\n",
        "        # Prototype i is the mean of all instances of features corresponding to labels == i\n",
        "        z_proto = torch.cat(\n",
        "            [\n",
        "                z_support[torch.nonzero(support_labels == label)].mean(0)\n",
        "                for label in range(n_way)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Compute the euclidean distance from queries to prototypes\n",
        "        dists = torch.cdist(z_query, z_proto)\n",
        "\n",
        "        # And here is the super complicated operation to transform those distances into classification scores!\n",
        "        scores = -dists\n",
        "        return scores\n",
        "\n",
        "\n",
        "convolutional_network = model\n",
        "convolutional_network.fc = nn.Flatten()\n",
        "print(convolutional_network)\n",
        "\n",
        "model = PrototypicalNetworks(convolutional_network).to(device)\n"
      ],
      "metadata": {
        "id": "8g6Qdx_nULxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_WAY = 5  # Number of classes in a task\n",
        "N_SHOT = 5  # Number of images per class in the support set\n",
        "N_QUERY = 10  # Number of images per class in the query set\n",
        "N_EVALUATION_TASKS = 100\n",
        "\n",
        "# The sampler needs a dataset with a \"get_labels\" method. Check the code if you have any doubt!\n",
        "test_set.get_labels = lambda: [\n",
        "    instance[1] for instance in test_set._flat_character_images\n",
        "]\n",
        "test_sampler = TaskSampler(\n",
        "    test_set, n_way=N_WAY, n_shot=N_SHOT, n_query=N_QUERY, n_tasks=N_EVALUATION_TASKS\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_set,\n",
        "    batch_sampler=test_sampler,\n",
        "    num_workers=12,\n",
        "    pin_memory=True,\n",
        "    collate_fn=test_sampler.episodic_collate_fn,\n",
        ")"
      ],
      "metadata": {
        "id": "dRgzdVIvUR-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(\n",
        "    example_support_images,\n",
        "    example_support_labels,\n",
        "    example_query_images,\n",
        "    example_query_labels,\n",
        "    example_class_ids,\n",
        ") = next(iter(test_loader))\n",
        "\n",
        "plot_images(example_support_images, \"support images\", images_per_row=N_SHOT)\n",
        "plot_images(example_query_images, \"query images\", images_per_row=N_QUERY)\n"
      ],
      "metadata": {
        "id": "6FizvjuAUSDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "example_scores = model(\n",
        "    example_support_images.cuda(),\n",
        "    example_support_labels.cuda(),\n",
        "    example_query_images.cuda(),\n",
        ").detach()\n",
        "\n",
        "_, example_predicted_labels = torch.max(example_scores.data, 1)\n",
        "\n",
        "print(\"Ground Truth / Predicted\")\n",
        "for i in range(len(example_query_labels)):\n",
        "    print(\n",
        "        f\"{test_set._characters[example_class_ids[example_query_labels[i]]]} / {test_set._characters[example_class_ids[example_predicted_labels[i]]]}\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "F_d4B3t3UU1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_on_one_task(\n",
        "    support_images: torch.Tensor,\n",
        "    support_labels: torch.Tensor,\n",
        "    query_images: torch.Tensor,\n",
        "    query_labels: torch.Tensor,\n",
        ") -> [int, int]:\n",
        "    \"\"\"\n",
        "    Returns the number of correct predictions of query labels, and the total number of predictions.\n",
        "    \"\"\"\n",
        "    return (\n",
        "        torch.max(\n",
        "            model(support_images.cuda(), support_labels.cuda(), query_images.cuda())\n",
        "            .detach()\n",
        "            .data,\n",
        "            1,\n",
        "        )[1]\n",
        "        == query_labels.cuda()\n",
        "    ).sum().item(), len(query_labels)\n",
        "\n",
        "\n",
        "def evaluate(data_loader: DataLoader):\n",
        "    # We'll count everything and compute the ratio at the end\n",
        "    total_predictions = 0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    # eval mode affects the behaviour of some layers (such as batch normalization or dropout)\n",
        "    # no_grad() tells torch not to keep in memory the whole computational graph (it's more lightweight this way)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for episode_index, (\n",
        "            support_images,\n",
        "            support_labels,\n",
        "            query_images,\n",
        "            query_labels,\n",
        "            class_ids,\n",
        "        ) in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
        "\n",
        "            correct, total = evaluate_on_one_task(\n",
        "                support_images, support_labels, query_images, query_labels\n",
        "            )\n",
        "\n",
        "            total_predictions += total\n",
        "            correct_predictions += correct\n",
        "\n",
        "    print(\n",
        "        f\"Model tested on {len(data_loader)} tasks. Accuracy: {(100 * correct_predictions/total_predictions):.2f}%\"\n",
        "    )\n",
        "\n",
        "\n",
        "evaluate(test_loader)"
      ],
      "metadata": {
        "id": "yFqjv3RiUXvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_TRAINING_EPISODES = 40000\n",
        "N_VALIDATION_TASKS = 100\n",
        "\n",
        "train_set.get_labels = lambda: [instance[1] for instance in train_set._flat_character_images]\n",
        "train_sampler = TaskSampler(\n",
        "    train_set, n_way=N_WAY, n_shot=N_SHOT, n_query=N_QUERY, n_tasks=N_TRAINING_EPISODES\n",
        ")\n",
        "train_loader = DataLoader(\n",
        "    train_set,\n",
        "    batch_sampler=train_sampler,\n",
        "    num_workers=12,\n",
        "    pin_memory=True,\n",
        "    collate_fn=train_sampler.episodic_collate_fn,\n",
        ")"
      ],
      "metadata": {
        "id": "aL9Otd1PUd9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "def fit(\n",
        "    support_images: torch.Tensor,\n",
        "    support_labels: torch.Tensor,\n",
        "    query_images: torch.Tensor,\n",
        "    query_labels: torch.Tensor,\n",
        ") -> float:\n",
        "    optimizer.zero_grad()\n",
        "    classification_scores = model(\n",
        "        support_images.cuda(), support_labels.cuda(), query_images.cuda()\n",
        "    )\n",
        "\n",
        "    loss = criterion(classification_scores, query_labels.cuda())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()"
      ],
      "metadata": {
        "id": "ol38LzVXUgMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model yourself with this cell\n",
        "\n",
        "log_update_frequency = 10\n",
        "\n",
        "all_loss = []\n",
        "model.train()\n",
        "with tqdm(enumerate(train_loader), total=len(train_loader)) as tqdm_train:\n",
        "    for episode_index, (\n",
        "        support_images,\n",
        "        support_labels,\n",
        "        query_images,\n",
        "        query_labels,\n",
        "        _,\n",
        "    ) in tqdm_train:\n",
        "        loss_value = fit(support_images, support_labels, query_images, query_labels)\n",
        "        all_loss.append(loss_value)\n",
        "\n",
        "        if episode_index % log_update_frequency == 0:\n",
        "            tqdm_train.set_postfix(loss=sliding_average(all_loss, log_update_frequency))\n"
      ],
      "metadata": {
        "id": "VEcfXIFXUiqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Or just load mine\n",
        "\n",
        "!wget https://public-sicara.s3.eu-central-1.amazonaws.com/easy-fsl/resnet18_with_pretraining.tar\n",
        "model.load_state_dict(torch.load(\"resnet18_with_pretraining.tar\", map_location=\"cuda\"))\n"
      ],
      "metadata": {
        "id": "WPglMjZHUjTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(test_loader)\n"
      ],
      "metadata": {
        "id": "SwLCcSDWUmO_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}